{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you have very large rdd to operate upon then before calling any of the below :\n",
    "#    join(), cogroup(), groupWith(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey() \n",
    "#    and lookup() then make sure to call .partitionBy() beforehand to split your data manually so that it can be \n",
    "#    distributed evenly on to the cluster nodes.\n",
    "\n",
    "# If you don't call .partitionBy() then some of your cluster nodes/ worker nodes would hang up due to heavy load for \n",
    "# running heavy job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As you know that we applied self join opeartion on rdd. If we have 1 million ratings dataset then applying self join\n",
    "# operation is very very costly. So, running that .join() operation on huge rdd won't work, even on 10 node cluster.\n",
    "# Because Spark won't distribute job by its own. You have to do a little work manually by applying .partitionBy()\n",
    "# method.\n",
    "\n",
    "# So Spark is not totally magic --> you need to think about how your data is partitioned.\n",
    "\n",
    "# join(), cogroup(), groupWith(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey() and \n",
    "# lookup() --> these operations when applied after .PartitionBy(), will preserve the partitioning in their results too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPARTITIONING : it is a way to optimize the job execution on large cluster.\\n\\nSo before appying join(), cogroup(), groupWith(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), \\ncombineByKey() and lookup() operations on rdd, always think that Is RDD huge? If so, then make sure to apply\\n.partitionBy() method before applying above opeartions on RDD.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "PARTITIONING : it is a way to optimize the job execution on large cluster.\n",
    "\n",
    "So before appying join(), cogroup(), groupWith(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), \n",
    "combineByKey() and lookup() operations on rdd, always think that Is RDD huge? If so, then make sure to apply\n",
    ".partitionBy() method before applying above opeartions on RDD.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nChoosing the partition size : \\n    a) Too less partitions won't take advantage of your whole cluster.\\n    b) Too many partitions would be an overhead from shuffling data.\\n    c) So patitions should be at least equal to the cores on all nodes or resources. Eg: If you have 10 node cluster \\n        with 4 cores on each node then partition size of 50 or doubling/thrice would be fine(100/200).\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Choosing the partition size : \n",
    "    a) Too less partitions won't take advantage of your whole cluster.\n",
    "    b) Too many partitions would be an overhead from shuffling data.\n",
    "    c) So patitions should be at least equal to the cores on all nodes or resources. Eg: If you have 10 node cluster \n",
    "        with 4 cores on each node then partition size of 50 or doubling/thrice would be fine(100/200).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyspark --master yarn --executor-memory 1g <script> <argv>\n",
    "\n",
    "# above script is to tell that each executor would make use of 1 GB of memory and cluster manager would be YARN\n",
    "\n",
    "# by default settings are : --executor-memory 500 mb --master local[*] <script> <argv>\n",
    "# i.e each executor will take 500 MB of RAM and cluster manager would be default cluster manager i.e sparks' built-in\n",
    "# cluster manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Amazon S3 is for storage on cloud, it's not for any computation.\n",
    "\n",
    "# Amazon EC2 is for computation on cloud, even Amazon EMR(elastic map reduce) makes use of EC2 internally.\n",
    "# To make cluster of nodes, so that we can run spark job on it, we could make use of Amazon EMR service. Amazon EMR\n",
    "# service uses hadoop YARN cluster manager as a default cluster managaer and gives each executor a 500 MB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
